{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# \"Attention is all you need\" - An extended overview"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attention Mechanism"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Screenshot](img.jpg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_580/762390872.py\", line 8, in <module>\n",
      "    import nltk\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/nltk/__init__.py\", line 132, in <module>\n",
      "    from nltk.collocations import *\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/nltk/collocations.py\", line 36, in <module>\n",
      "    from nltk.metrics import (\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/nltk/metrics/__init__.py\", line 18, in <module>\n",
      "    from nltk.metrics.association import (\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/nltk/metrics/association.py\", line 26, in <module>\n",
      "    from scipy.stats import fisher_exact\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/scipy/stats/__init__.py\", line 485, in <module>\n",
      "    from ._stats_py import *\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/scipy/stats/_stats_py.py\", line 37, in <module>\n",
      "    from numpy.testing import suppress_warnings\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/numpy/testing/__init__.py\", line 11, in <module>\n",
      "    from ._private.utils import *\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/numpy/testing/_private/utils.py\", line 417, in <module>\n",
      "    @np._no_nep50_warning()\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/numpy/__init__.py\", line 311, in __getattr__\n",
      "AttributeError: module 'numpy' has no attribute '_no_nep50_warning'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1428, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1319, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1172, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1087, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 969, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/eduardburlacu/miniconda3/envs/tf/lib/python3.9/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization\n",
    "import string\n",
    "import re\n",
    "from random import shuffle\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T22:49:43.781247200Z",
     "start_time": "2023-06-25T22:49:43.352685200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SelfAttention(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "    def call(self, inputs, training=False, mask=False):\n",
    "        \"\"\"\"\n",
    "        Feed-forward action: Feed Q, K in the normalised matrix multiplication, optionally mask for causality, then apply softmax to map into conditional probabilities.\n",
    "        Dimensions:\n",
    "        - inputs: Q,K = (N, dk), V=(N, dv)\n",
    "        - inputs: Q,K = (N, dk), V=(N, dv)\n",
    "        - output: (N, dv)\n",
    "        \"\"\"\n",
    "        Q, K, V = inputs\n",
    "        self.dk = K.shape[1]\n",
    "        self.dv = V.shape[1]\n",
    "        x = tf.linalg.matmul(Q, tf.transpose(K)) / tf.sqrt(self.dk)\n",
    "        if mask: x = layers.Masking()(x)\n",
    "        x = layers.Softmax()(x)\n",
    "        return tf.linalg.matmul(x, V)\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, heads=8, dk=64 ,dv=64, dmodel=512):\n",
    "        assert (dk% heads == 0)\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.h = heads\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.dmodel = dmodel\n",
    "        self.dense_Q = [layers.Dense(dk) for _ in range(heads)]\n",
    "        self.dense_K = [layers.Dense(dk) for _ in range(heads)]\n",
    "        self.dense_V = [layers.Dense(dv) for _ in range(heads)]\n",
    "        self.W0 = layers.Dense(dmodel)\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        Q,K,V = inputs\n",
    "        xQ=[None] * self.h\n",
    "        xK=[None] * self.h\n",
    "        xV=[None] * self.h\n",
    "        x =[None] * self.h\n",
    "        for i in range(self.h):\n",
    "            xQ[i] = self.dense_Q[i](Q)\n",
    "            xK[i] = self.dense_K[i](K)\n",
    "            xV[i] = self.dense_V[i](V)\n",
    "            x[i]  = SelfAttention()([xQ[i],xK[i],xV[i]])\n",
    "        x = layers.Concatenate()(x)\n",
    "        x = self.W0(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, heads=8, dk=64 ,dv=64, dmodel=512, dff=2048, name='Encoder',**kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dmodel= dmodel\n",
    "        self.dff=dff\n",
    "        self.MHA  = MultiHeadAttention(heads, dk ,dv, dmodel)\n",
    "        self.dense1 = layers.Dense(dff,activation='relu')\n",
    "        self.dense2 = layers.Dense(dmodel)\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        x = self.MHA([inputs,inputs,inputs])\n",
    "        x = layers.Add()([x, inputs])\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        y = layers.Dropout(rate=0.1)(x)\n",
    "        y = self.dense1(x)\n",
    "        y = layers.Dropout(rate=0.1)(y)\n",
    "        y = self.dense2(y)\n",
    "        x = layers.Add()([x, y])\n",
    "        x = layers.BatchNormalization(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, heads=8, dk=64 ,dv=64, dmodel=512,dff=2048, name='Decoder',**kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dmodel= dmodel\n",
    "        self.dff=dff\n",
    "        self.MHA1 = MultiHeadAttention(heads, dk ,dv, dmodel)\n",
    "        self.MHA2 = MultiHeadAttention(heads, dk ,dv, dmodel)\n",
    "        self.dense1 = layers.Dense(dff,activation='relu')\n",
    "        self.dense2 = layers.Dense(dmodel)\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        encoded, recurrent = inputs\n",
    "        x= self.MHA1([recurrent, recurrent, recurrent])\n",
    "        x = x+recurrent\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        y = self.MHA2([encoded, encoded, x])\n",
    "        x = layers.Add()([x, y])\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        z = layers.Dropout(rate=0.1)(x)\n",
    "        z = self.dense1(x)\n",
    "        z = layers.Dropout(rate=0.1)(x)\n",
    "        z = self.dense2(z)\n",
    "        x = layers.Add()([x,z])\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Transformer](Transformer.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, name='Positional Encoder'):\n",
    "        super(PositionalEncoding, self).__init__(name=name)\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        dmodel = inputs.shape[1]\n",
    "        x = tf.range(limit=dmodel )\n",
    "        condition = x%2\n",
    "        pos = x - condition\n",
    "        a = tf.math.sin(x/(10000**(pos/dmodel)))\n",
    "        b = tf.math.cos(x/(10000**(pos/dmodel)))\n",
    "        return tf.where(condition=condition, x=a, y=b)\n",
    "\n",
    "class Transformer(keras.Model):\n",
    "    def __init__(self, num_encoders=6, heads=8, dk=64 ,dv=64, dmodel=512, dff=2048, vocab_size=500, max_seq_length=200 ,name='Transformer', **kwargs):\n",
    "        super(Transformer, self).__init__(name=name, **kwargs)\n",
    "\n",
    "        self.Encoder = [Encoder(heads=heads, dk=dk ,dv=dv, dmodel=dmodel,dff=dff) for _ in range(num_encoders)]\n",
    "\n",
    "        self.Decoder = [Decoder(heads=heads, dk=dk ,dv=dv, dmodel=dmodel,dff=dff) for _ in range(num_encoders)]\n",
    "\n",
    "        self.inEmbedding = layers.Embedding(input_dim = vocab_size, output_dim = dmodel, input_length=max_seq_length)\n",
    "\n",
    "        self.outEmbedding= layers.Embedding(input_dim = vocab_size, output_dim = dmodel, input_length=max_seq_length)\n",
    "\n",
    "        self.dense = layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "        self.positional = PositionalEncoding()\n",
    "\n",
    "        self.num_encoders= num_encoders\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        question, start_token, end_token= inputs\n",
    "        x = self.inEmbedding(question)\n",
    "        y = self.positional(x)\n",
    "        x = layers.Add()([x,y])\n",
    "        for i in range(self.num_encoders):\n",
    "            x = self.Encoder[i](x)\n",
    "        z = start_token\n",
    "        output=[]\n",
    "        while True:\n",
    "            z = self.outEmbedding(z)\n",
    "            t = self.positional(z)\n",
    "            z = layers.Add()([z, t])\n",
    "            for i in range(self.num_encoders):\n",
    "                z = self.Decoder[i]([x ,z])\n",
    "                z = self.dense(z)\n",
    "            output.append(z)\n",
    "            if (z==end_token):\n",
    "                break\n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Text Extracter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs=[]\n",
    "with open('archive/spa.txt','r') as file:\n",
    "    lines = file.read().split(sep='\\n')\n",
    "    for line in lines:\n",
    "        line_split = line.split('\\t')\n",
    "        try:\n",
    "            eng, spn = line_split[:2]\n",
    "            spn = '[start] ' + spn + ' [end]'\n",
    "            inputs.append([eng,spn])\n",
    "        except: pass\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N = len(inputs)\n",
    "Ntrain, Nval, Ntest = int(.65 * N), int(.15 * N), int(.2 * N)\n",
    "shuffle(inputs)\n",
    "train_set = inputs[:Ntrain]\n",
    "val_set  = inputs[Ntrain:Nval+Ntrain]\n",
    "test_set = inputs[Nval+Ntrain:]\n",
    "\n",
    "train_eng = [x[0] for x in train_set]\n",
    "train_spn = [x[1] for x in train_set]\n",
    "\n",
    "val_eng = [x[0] for x in val_set]\n",
    "val_spn = [x[1] for x in val_set]\n",
    "\n",
    "test_eng = [x[0] for x in test_set]\n",
    "test_spn = [x[0] for x in test_set]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_vocabulary_size(sequences):\n",
    "    corpus = ' '.join(sequences) # Combine all sequences into a single text\n",
    "    tokens = nltk.word_tokenize(corpus) # Tokenize the text\n",
    "    vocabulary_size = len(set(tokens)) # Count the unique tokens\n",
    "    return vocabulary_size\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "eng = [ tup[0] for tup in inputs ]\n",
    "spn = [ tup[1] for tup in inputs ]\n",
    "\n",
    "eng_vocab_size = get_vocabulary_size(eng)\n",
    "spn_vocab_size = get_vocabulary_size(spn)\n",
    "\n",
    "eng_seq_length = max(\n",
    "    map(lambda x: len(x.split()), eng)\n",
    ")\n",
    "\n",
    "spn_seq_length = max(\n",
    "    map(lambda x: len(x.split()), spn)\n",
    ")\n",
    "\n",
    "seq_length = max(eng_seq_length, spn_seq_length)\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "strip_chars = string.punctuation + '¿'\n",
    "strip_chars = strip_chars.replace('[','')\n",
    "strip_chars = strip_chars.replace(']','')\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=eng_vocab_size, output_mode='int', output_sequence_length = seq_length+1, standardize= custom_standardization\n",
    ")\n",
    "\n",
    "spn_vectorization = TextVectorization(\n",
    "    max_tokens=spn_vocab_size, output_mode='int', output_sequence_length = seq_length+1, standardize= custom_standardization\n",
    ")\n",
    "\n",
    "eng_vectorization.adapt()\n",
    "spn_vectorization.adapt()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
